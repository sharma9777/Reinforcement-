{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOOO8tYlaiom",
        "outputId": "6f6a6e25-094a-4dbb-87ac-e6451f6105c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.11.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.0 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install gymnasium[atari] gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y gymnasium ale-py\n",
        "!pip install gymnasium==0.29.1 ale-py==0.8.1\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kk_MHzmUiUjR",
        "outputId": "8444bdf3-591a-4932-f145-f720aef2a4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gymnasium 1.2.0\n",
            "Uninstalling gymnasium-1.2.0:\n",
            "  Successfully uninstalled gymnasium-1.2.0\n",
            "Found existing installation: ale-py 0.11.2\n",
            "Uninstalling ale-py-0.11.2:\n",
            "  Successfully uninstalled ale-py-0.11.2\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ale-py==0.8.1\n",
            "  Downloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py==0.8.1) (6.5.2)\n",
            "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, ale-py\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.8.1 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.67.1)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2025.7.14)\n",
            "Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=autorom_accept_rom_license-0.6.1-py3-none-any.whl size=446709 sha256=d3329dafe35e68e6f3014403ba12d71c140e15fd0d134bae13097e9d3da716e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/fc/c6/8aa657c0d2089982f2dabd110efc68c61eb49831fdb7397351\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y gymnasium ale-py\n",
        "!pip install gymnasium==0.29.1 ale-py==0.8.1\n",
        "!pip install gymnasium[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niaHiVopjQQT",
        "outputId": "04878c0a-daaf-4bd0-af17-722daccd2bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gymnasium 0.29.1\n",
            "Uninstalling gymnasium-0.29.1:\n",
            "  Successfully uninstalled gymnasium-0.29.1\n",
            "Found existing installation: ale-py 0.8.1\n",
            "Uninstalling ale-py-0.8.1:\n",
            "  Successfully uninstalled ale-py-0.8.1\n",
            "Collecting gymnasium==0.29.1\n",
            "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting ale-py==0.8.1\n",
            "  Using cached ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py==0.8.1) (6.5.2)\n",
            "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "Using cached ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Installing collected packages: gymnasium, ale-py\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.8.1 gymnasium-0.29.1\n",
            "Requirement already satisfied: gymnasium[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[accept-rom-license]) (0.0.4)\n",
            "Requirement already satisfied: autorom~=0.4.2 in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (4.67.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license]) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Remove any partial installs\n",
        "!pip uninstall -y gymnasium ale-py AutoROM\n",
        "\n",
        "# 2) Re‑install Gymnasium *with* Atari support + auto‑accept the ROM licence\n",
        "!pip install \"gymnasium[atari,accept-rom-license]==0.29.1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tHjr_uKkzkT",
        "outputId": "900421bf-6abc-4f75-d927-9dcf678a4d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: gymnasium 0.29.1\n",
            "Uninstalling gymnasium-0.29.1:\n",
            "  Successfully uninstalled gymnasium-0.29.1\n",
            "Found existing installation: ale-py 0.8.1\n",
            "Uninstalling ale-py-0.8.1:\n",
            "  Successfully uninstalled ale-py-0.8.1\n",
            "Found existing installation: AutoROM 0.4.2\n",
            "Uninstalling AutoROM-0.4.2:\n",
            "  Successfully uninstalled AutoROM-0.4.2\n",
            "Collecting gymnasium==0.29.1 (from gymnasium[accept-rom-license,atari]==0.29.1)\n",
            "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[accept-rom-license,atari]==0.29.1) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[accept-rom-license,atari]==0.29.1) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[accept-rom-license,atari]==0.29.1) (4.14.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==0.29.1->gymnasium[accept-rom-license,atari]==0.29.1) (0.0.4)\n",
            "Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1)\n",
            "  Using cached AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]==0.29.1)\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (4.67.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (0.6.1)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]==0.29.1)\n",
            "  Using cached ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]==0.29.1) (6.5.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]==0.29.1) (2025.7.14)\n",
            "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "Using cached AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Using cached ale_py-0.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "Installing collected packages: gymnasium, ale-py, shimmy, autorom\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires ale-py>=0.10.1, but you have ale-py 0.8.1 which is incompatible.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ale-py-0.8.1 autorom-0.4.2 gymnasium-0.29.1 shimmy-0.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MLPEncoder(nn.Module):\n",
        "    def __init__(self, in_features, embed_dim=256):\n",
        "        super(MLPEncoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features, 128)\n",
        "        self.fc2 = nn.Linear(128, embed_dim)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, in_channels=4, embed_dim=256):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc1 = nn.Linear(3136, embed_dim) # 64 * 7 * 7 = 3136\n",
        "    def forward(self, x):\n",
        "        # Input tensor shape: (B, C, H, W). In our case (B, 4, 84, 84)\n",
        "        # Convert to float and scale pixel values\n",
        "        x = x.float() / 255.0\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, embed_dim=256, action_dim=4):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, action_dim)\n",
        "    def forward(self, z):\n",
        "        x = F.relu(self.fc1(z))\n",
        "        return self.fc2(x)\n",
        "\n",
        "class ForwardModel(nn.Module):\n",
        "    def __init__(self, embed_dim=256, action_dim=4):\n",
        "        super(ForwardModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim + action_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, embed_dim)\n",
        "    def forward(self, z, a_oh):\n",
        "        za = torch.cat([z, a_oh], dim=1)\n",
        "        x = F.relu(self.fc1(za))\n",
        "        return self.fc2(x)\n",
        "\n",
        "print(\"✅ Cell 1/3: Model classes defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk1442iZanxP",
        "outputId": "66321aaa-2ab2-4e39-a67a-aa4545c7f10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cell 1/3: Model classes defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "\n",
        "class RunningStats:\n",
        "    def __init__(self): self.n = 0; self.mean = 0.0; self.M2 = 0.0\n",
        "    def update(self, values):\n",
        "        for x in values:\n",
        "            self.n += 1\n",
        "            delta = x - self.mean\n",
        "            self.mean += delta / self.n\n",
        "            self.M2   += delta * (x - self.mean)\n",
        "    @property\n",
        "    def std(self): return 1.0 if self.n < 2 else (self.M2 / (self.n - 1)) ** 0.5\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity: int): self.buffer = deque(maxlen=capacity)\n",
        "    def add(self, s, a, r, s_next, done): self.buffer.append((s, a, r, s_next, done))\n",
        "    def sample(self, bs: int):\n",
        "        s, a, r, s2, d = zip(*random.sample(self.buffer, bs))\n",
        "        # For pixel data, np.array() is sufficient and often faster than np.vstack()\n",
        "        return (np.array(s), np.array(a), np.array(r), np.array(s2), np.array(d))\n",
        "    def __len__(self): return len(self.buffer)\n",
        "\n",
        "class MaxMinExplorerAgent:\n",
        "    def __init__(self, action_dim, device, encoder,\n",
        "                 N=3, M=5,\n",
        "                 alpha=1e-4, alpha_g=1e-4,\n",
        "                 gamma=0.99, epsilon=0.05,\n",
        "                 eta_Q=1.0, lambda_int=1.0,\n",
        "                 beta_min=0.05, dis_min=0.05,\n",
        "                 tau=0.005,\n",
        "                 buffer_capacity=1_000_000,\n",
        "                 batch_size=32, tau_pred=32,\n",
        "                 embed_dim=256):\n",
        "\n",
        "        self.device, self.N, self.M = device, N, M\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma, self.epsilon = gamma, epsilon\n",
        "        self.eta_Q, self.lambda_int = eta_Q, lambda_int\n",
        "        self.beta_min, self.dis_min = beta_min, dis_min\n",
        "        self.tau, self.batch_size, self.tau_pred = tau, batch_size, tau_pred\n",
        "        self.eps = 1e-8\n",
        "\n",
        "        self.encoder = encoder.to(device)\n",
        "        self.critics = [QNetwork(embed_dim, action_dim).to(device) for _ in range(N)]\n",
        "        self.target_critics = [QNetwork(embed_dim, action_dim).to(device) for _ in range(N)]\n",
        "        for tgt, src in zip(self.target_critics, self.critics):\n",
        "            tgt.load_state_dict(src.state_dict())\n",
        "        self.ensemble_models = [ForwardModel(embed_dim, action_dim).to(device) for _ in range(M)]\n",
        "\n",
        "        all_critic_params = sum((list(net.parameters()) for net in self.critics), [])\n",
        "        all_model_params = sum((list(m.parameters()) for m in self.ensemble_models), [])\n",
        "        self.opt_critic  = torch.optim.Adam(all_critic_params, lr=alpha)\n",
        "        self.opt_models  = torch.optim.Adam(all_model_params, lr=alpha_g)\n",
        "        self.opt_encoder = torch.optim.Adam(self.encoder.parameters(), lr=alpha)\n",
        "\n",
        "        self.replay = ReplayBuffer(buffer_capacity)\n",
        "        self.beta_stats, self.dis_stats = RunningStats(), RunningStats()\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.action_dim)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # For Atari, state is (4, 84, 84), need to add batch dim -> (1, 4, 84, 84)\n",
        "            s_t = torch.as_tensor(state, device=self.device).unsqueeze(0)\n",
        "            z   = self.encoder(s_t)\n",
        "            q_vals = torch.stack([c(z) for c in self.critics]).squeeze(1)\n",
        "\n",
        "            z_exp  = z.repeat(self.action_dim, 1)\n",
        "            a_ohs  = F.one_hot(torch.arange(int(self.action_dim), device=self.device),\n",
        "                                 num_classes=self.action_dim).float()\n",
        "            preds  = torch.stack([m(z_exp, a_ohs) for m in self.ensemble_models])\n",
        "\n",
        "            q_np   = q_vals.cpu().numpy()\n",
        "            raw_b  = self.eta_Q * np.var(q_np, axis=0)\n",
        "            raw_d  = torch.std(preds, dim=0).mean(dim=1).cpu().numpy()\n",
        "\n",
        "            self.beta_stats.update(raw_b); self.dis_stats.update(raw_d)\n",
        "\n",
        "            q_min = q_np.min(axis=0)\n",
        "            scores = []\n",
        "            for rb, rd, q in zip(raw_b, raw_d, q_min):\n",
        "                beta = max(rb / (self.beta_stats.std + self.eps), self.beta_min)\n",
        "                dis  = max(rd / (self.dis_stats.std + self.eps),  self.dis_min)\n",
        "                scores.append(q + self.lambda_int * beta * dis)\n",
        "\n",
        "            return int(np.argmax(scores))\n",
        "\n",
        "    def store_transition(self, s, a, r, s2, done):\n",
        "        self.replay.add(s, a, r, s2, done)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay) < self.batch_size: return\n",
        "        s, a, r, s2, d = self.replay.sample(self.batch_size)\n",
        "        s    = torch.as_tensor(s,  device=self.device)\n",
        "        s2   = torch.as_tensor(s2, device=self.device)\n",
        "        r    = torch.as_tensor(r,  dtype=torch.float32, device=self.device)\n",
        "        done = torch.as_tensor(d,  dtype=torch.float32, device=self.device)\n",
        "        a    = torch.as_tensor(a,  dtype=torch.long,    device=self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z2 = self.encoder(s2)\n",
        "            proposer_idx = random.randrange(self.N)\n",
        "            q_proposals = self.critics[proposer_idx](z2)\n",
        "            a_star = q_proposals.argmax(dim=1)\n",
        "\n",
        "            q_vals_all_targets = torch.stack([qc(z2) for qc in self.target_critics])\n",
        "            a_star_expanded = a_star.view(1, -1, 1).expand(self.N, -1, -1)\n",
        "            q_vals_for_astar = q_vals_all_targets.gather(2, a_star_expanded).squeeze(-1)\n",
        "\n",
        "            q_min_target = q_vals_for_astar.min(dim=0).values\n",
        "            y = r + self.gamma * q_min_target * (1 - done)\n",
        "\n",
        "        z      = self.encoder(s)\n",
        "        a_oh   = F.one_hot(a, self.action_dim).float()\n",
        "        pred_all_q = self.critics[proposer_idx](z)\n",
        "        pred   = pred_all_q.gather(1, a.unsqueeze(1))\n",
        "        loss_q = F.mse_loss(pred, y.unsqueeze(1))\n",
        "\n",
        "        self.opt_encoder.zero_grad(); self.opt_critic.zero_grad()\n",
        "        loss_q.backward(); self.opt_critic.step(); self.opt_encoder.step()\n",
        "\n",
        "        if self.tau_pred and len(self.replay) > self.tau_pred:\n",
        "            idxs   = np.random.choice(self.batch_size, self.tau_pred, replace=False)\n",
        "            z_sel  = z[idxs].detach()\n",
        "            with torch.no_grad(): z2_sel = self.encoder(s2[idxs])\n",
        "            a_sel = a_oh[idxs]\n",
        "\n",
        "            loss_g = sum(F.mse_loss(m(z_sel, a_sel), z2_sel) for m in self.ensemble_models) / self.M\n",
        "            self.opt_models.zero_grad(); loss_g.backward(); self.opt_models.step()\n",
        "\n",
        "        for tgt, src in zip(self.target_critics, self.critics):\n",
        "            for tp, p in zip(tgt.parameters(), src.parameters()):\n",
        "                tp.data.mul_(1 - self.tau).add_(self.tau * p.data)\n",
        "\n",
        "print(\"✅ Cell 2/3: Agent class defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7RextBIanzw",
        "outputId": "787c56c5-3cdb-4c6b-9784-f5be8c67ff96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Cell 2/3: Agent class defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "#  Cell 3: train_breakout.py\n",
        "# ================================================================\n",
        "import gymnasium as gym\n",
        "import torch, random, time, os\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 1. Configuration for the Breakout experiment\n",
        "# ------------------------------------------------\n",
        "class SweepConfig:\n",
        "    # ▶️ Pick any ID that shows up in gym.envs.registry\n",
        "    #    e.g., \"BreakoutNoFrameskip-v4\"  (legacy)  or\n",
        "    #          \"ALE/Breakout-v5\"         (new Farama namespace)\n",
        "    env_name = \"BreakoutNoFrameskip-v4\"\n",
        "\n",
        "    lambda_ints_to_test = [0.0, 0.05, 0.1, 0.2]\n",
        "    etas_to_test        = [1.0, 0.5]\n",
        "\n",
        "    # Atari‑specific hyper‑parameters\n",
        "    total_steps  = 1_000_000\n",
        "    buffer_size  = 200_000\n",
        "    warmup_steps = 5_000\n",
        "\n",
        "    # Misc\n",
        "    render_mode = None\n",
        "    seed        = 42\n",
        "    embed_dim   = 256\n",
        "    batch_size  = 32\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 2. Experiment and sweep logic\n",
        "# ------------------------------------------------\n",
        "def run_experiment(config: SweepConfig):\n",
        "    # ---- Reproducibility\n",
        "    random.seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(config.seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # ---- Create env -------------------------------------------------\n",
        "    # If we picked an ALE/…‑v5 ID, pass frameskip=1 to keep parity with\n",
        "    # the legacy “NoFrameskip‑v4” versions.\n",
        "    make_kwargs = dict(render_mode=config.render_mode)\n",
        "    if config.env_name.startswith(\"ALE/\"):\n",
        "        make_kwargs[\"frameskip\"] = 1\n",
        "\n",
        "    env = gym.make(config.env_name, **make_kwargs)\n",
        "\n",
        "    print(f\"Using env ID: {config.env_name}\")\n",
        "    print(\"Applying AtariPreprocessing + FrameStack …\")\n",
        "\n",
        "    env = gym.wrappers.AtariPreprocessing(\n",
        "        env, frame_skip=1, screen_size=84,\n",
        "        grayscale_obs=True, scale_obs=False, noop_max=30\n",
        "    )\n",
        "    env = gym.wrappers.FrameStack(env, 4)\n",
        "\n",
        "    # ---- Networks / Agent ------------------------------------------\n",
        "    encoder     = CNNEncoder(in_channels=4, embed_dim=config.embed_dim)\n",
        "    action_dim  = env.action_space.n\n",
        "\n",
        "    agent = MaxMinExplorerAgent(\n",
        "        action_dim=action_dim,\n",
        "        device=device,\n",
        "        encoder=encoder,\n",
        "        buffer_capacity=config.buffer_size,\n",
        "        batch_size=config.batch_size,\n",
        "        embed_dim=config.embed_dim,\n",
        "        lambda_int=config.lambda_int,\n",
        "        eta_Q=config.eta_Q,\n",
        "    )\n",
        "\n",
        "    reward_history = deque(maxlen=100)\n",
        "\n",
        "    # ---- Warm‑up ----------------------------------------------------\n",
        "    print(f\"[INFO] Warming up replay buffer for {config.warmup_steps} steps …\")\n",
        "    state, _ = env.reset()\n",
        "    for _ in range(config.warmup_steps):\n",
        "        a = env.action_space.sample()\n",
        "        s2, r, term, trunc, _ = env.step(a)\n",
        "        agent.store_transition(state, a, r, s2, term or trunc)\n",
        "        state, _ = env.reset() if term or trunc else (s2, None)\n",
        "\n",
        "    # ---- Main loop --------------------------------------------------\n",
        "    print(f\"[INFO] Starting training for {config.total_steps:,} steps …\")\n",
        "    state, _          = env.reset()\n",
        "    episode_reward    = 0\n",
        "    episode_count     = 1\n",
        "\n",
        "    for step in range(1, config.total_steps + 1):\n",
        "        a = agent.choose_action(state)\n",
        "        s2, r, term, trunc, _ = env.step(a)\n",
        "        done = term or trunc\n",
        "\n",
        "        agent.store_transition(state, a, r, s2, done)\n",
        "        agent.learn()\n",
        "\n",
        "        state          = s2\n",
        "        episode_reward += r\n",
        "\n",
        "        if step % 25_000 == 0:\n",
        "            avg100 = np.mean(reward_history) if reward_history else 0.0\n",
        "            print(f\"Step {step:7d}/{config.total_steps:,} | \"\n",
        "                  f\"Episodes {episode_count:5d} | Avg100 {avg100:5.1f}\")\n",
        "\n",
        "        if done:\n",
        "            reward_history.append(episode_reward)\n",
        "            state, _      = env.reset()\n",
        "            episode_reward = 0\n",
        "            episode_count += 1\n",
        "\n",
        "    return np.mean(reward_history) if reward_history else -999.0\n",
        "\n",
        "\n",
        "# ------------------------------------------------\n",
        "# 3. Hyper‑parameter sweep\n",
        "# ------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    results = {}\n",
        "    cfg      = SweepConfig()\n",
        "\n",
        "    for lam in cfg.lambda_ints_to_test:\n",
        "        for eta in cfg.etas_to_test:\n",
        "            run_key = f\"lambda={lam}_eta={eta}\"\n",
        "            print(\"\\n\" + \"=\" * 46)\n",
        "            print(f\"🚀 STARTING RUN: {run_key}\")\n",
        "            print(\"=\" * 46 + \"\\n\")\n",
        "\n",
        "            cfg.lambda_int = lam\n",
        "            cfg.eta_Q      = eta\n",
        "\n",
        "            t0   = time.time()\n",
        "            score = run_experiment(cfg)\n",
        "            t1   = time.time()\n",
        "\n",
        "            results[run_key] = score\n",
        "            print(f\"\\n--- FINAL AVG SCORE for {run_key}: {score:.2f} \"\n",
        "                  f\"(elapsed {(t1 - t0)/60:.1f} min) ---\")\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\\n🏆 SWEEP COMPLETE 🏆\")\n",
        "    best = max(results, key=results.get)\n",
        "    print(f\"Best run: {best}  |  Avg100 = {results[best]:.2f}\\n\")\n",
        "    print(\"Full results:\")\n",
        "    for k, v in sorted(results.items(), key=lambda kv: kv[1], reverse=True):\n",
        "        print(f\"  {k}: {v:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J6vIWeIan2D",
        "outputId": "1af8debe-306f-418f-a568-f6d47bd1e5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==============================================\n",
            "🚀 STARTING RUN: lambda=0.0_eta=1.0\n",
            "==============================================\n",
            "\n",
            "Using env ID: BreakoutNoFrameskip-v4\n",
            "Applying AtariPreprocessing + FrameStack …\n",
            "[INFO] Warming up replay buffer for 5000 steps …\n",
            "[INFO] Starting training for 1,000,000 steps …\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-6-2894992543.py:70: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  s_t = torch.as_tensor(state, device=self.device).unsqueeze(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step   25000/1,000,000 | Episodes    28 | Avg100   1.9\n",
            "Step   50000/1,000,000 | Episodes    54 | Avg100   1.8\n",
            "Step   75000/1,000,000 | Episodes    80 | Avg100   1.8\n",
            "Step  100000/1,000,000 | Episodes   105 | Avg100   1.9\n",
            "Step  125000/1,000,000 | Episodes   128 | Avg100   2.2\n",
            "Step  150000/1,000,000 | Episodes   150 | Avg100   2.5\n",
            "Step  175000/1,000,000 | Episodes   172 | Avg100   2.9\n",
            "Step  200000/1,000,000 | Episodes   192 | Avg100   3.4\n",
            "Step  225000/1,000,000 | Episodes   211 | Avg100   3.9\n",
            "Step  250000/1,000,000 | Episodes   230 | Avg100   4.3\n",
            "Step  275000/1,000,000 | Episodes   246 | Avg100   4.8\n",
            "Step  300000/1,000,000 | Episodes   263 | Avg100   5.1\n",
            "Step  325000/1,000,000 | Episodes   280 | Avg100   5.4\n",
            "Step  350000/1,000,000 | Episodes   298 | Avg100   5.6\n",
            "Step  375000/1,000,000 | Episodes   315 | Avg100   5.9\n",
            "Step  400000/1,000,000 | Episodes   330 | Avg100   6.1\n",
            "Step  425000/1,000,000 | Episodes   346 | Avg100   6.4\n",
            "Step  450000/1,000,000 | Episodes   361 | Avg100   6.6\n",
            "Step  475000/1,000,000 | Episodes   378 | Avg100   6.7\n",
            "Step  500000/1,000,000 | Episodes   391 | Avg100   7.0\n",
            "Step  525000/1,000,000 | Episodes   406 | Avg100   7.1\n",
            "Step  550000/1,000,000 | Episodes   423 | Avg100   7.2\n",
            "Step  575000/1,000,000 | Episodes   438 | Avg100   7.3\n",
            "Step  600000/1,000,000 | Episodes   453 | Avg100   7.3\n",
            "Step  625000/1,000,000 | Episodes   467 | Avg100   7.4\n",
            "Step  650000/1,000,000 | Episodes   482 | Avg100   7.6\n",
            "Step  675000/1,000,000 | Episodes   497 | Avg100   7.6\n",
            "Step  700000/1,000,000 | Episodes   512 | Avg100   7.7\n",
            "Step  725000/1,000,000 | Episodes   530 | Avg100   7.4\n",
            "Step  750000/1,000,000 | Episodes   544 | Avg100   7.5\n",
            "Step  775000/1,000,000 | Episodes   562 | Avg100   7.1\n",
            "Step  800000/1,000,000 | Episodes   576 | Avg100   7.2\n",
            "Step  825000/1,000,000 | Episodes   592 | Avg100   7.0\n",
            "Step  850000/1,000,000 | Episodes   609 | Avg100   6.8\n",
            "Step  875000/1,000,000 | Episodes   622 | Avg100   7.3\n",
            "Step  900000/1,000,000 | Episodes   636 | Avg100   7.5\n",
            "Step  925000/1,000,000 | Episodes   652 | Avg100   7.6\n",
            "Step  950000/1,000,000 | Episodes   667 | Avg100   7.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KB-VjfAOiSxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iX7FwtqWiSzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iWX61RvaiS28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7j8zFo78an6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n0HV_w1Qan9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fnr7nk7man_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SgFauy8KaoCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JGTp1f7EaoFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1iIl5HndaoHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QmSDlPBjaoJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oY63jRTCaoNa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}